library(sf)
library(DBI)
library(RPostgres)
con <- dbConnect(
  RPostgres::Postgres(),
  dbname = "proyecto_12",
  host="proyecto12.czowmm2ee3ym.us-east-1.rds.amazonaws.com",
  port=5432,
  user="postgres",
  password="proyecto12upc",
)

dbListTables(con)
df <- dbGetQuery(con, "SELECT * FROM calc_neighborhoods_consolidated")

## Normalidad total_stations_bikes_output

```{r}
hist(df$total_stations_bikes_output)

mm<-mean(df$total_stations_bikes_output);ss<-sd(df$total_stations_bikes_output)
mm;ss
hist(df$total_stations_bikes_output,freq=F)
curve(dnorm(x,mm,ss),col="red",add=T)
shapiro.test(df$total_stations_bikes_output) # No hay normalidad

## Descriptiva fare, dist, concen, passen

```{r}
names(df)
selcol<-c(1:5,7)
summary(df[,selcol])
```

## Variables más relacionadas con total_stations_bikes_output

```{r}
plot(df[,selcol])
# Mejor
plot(df[,c(7,5,6,15)])  # Herramienta gráfica
# Calcular coeficiente correlación lineal (Pearson)
cor(df[,c(7,5,6,15)])

``````


#Verificacion de NAs y columnas no numericas:
str(df)
summary(df)

colSums(is.na(df))
colSums(is.nan(as.matrix(df)))
cor(df, use = "pairwise.complete.obs")
#alta correlacion entre población y poblacion de mujeres. También entre capacity y total_bike_output.
install.packages("car")
library(car)
df$population <- as.numeric(df$population)
unique(df$population)
#conversión a numérico porque no reconocía la varianza de population

vif(lm(total_bikes_output ~ population + avg_age + avg_income + avg_altitude, data = df))
#Solo me permitió calcular el vif habiendo quitado capacity y poblacion de mujeres, entiendo que por la colinealidad
#(aparecía el error "there are aliased coefficients in the model")

modelo_lm <- lm(total_bikes_output ~ population + avg_age + avg_income + avg_altitude, data = df)
summary(modelo_lm)


###Analisis modelo

summary(df)
warnings()
cor(df[, c(1,5,6,7,8,12,15)])
plot(df[,c(1,5,6,7,8,12,15)])
#la variable population no aparece en las correlaciones
class(df$population)
df$population <- as.numeric(df$population)
#despues de convertir population a numerica, se pudieron hacer todas las correlaciones
#sapply(df[, c(1, 5, 6, 7, 8, 12, 15)], function(x) sd(x, na.rm = TRUE))



library(FactoMineR)
df$neighborhood_code <- as.factor(df$neighborhood_code)
df_numeric <- df[, sapply(df, is.numeric)]
df_subset <- df[, c(1, 5, 6, 7, 8, 12, 15)]

# Convertir la variable de agrupación a factor
df_subset$neighborhood_code <- as.factor(df_subset$neighborhood_code)

# Ejecutar condes() usando la primera columna (la variable de agrupación)
condes(df_subset, 1)
modelo <- lm(total_stations_bikes_output ~ population + perc_women + average_age + gross_income + stations_avg_altitude, data = df)
summary(modelo)


library(chemometrics)

# Usamos las columnas 2 a 7 (las variables continuas)
res.mout <- Moutlier(df_subset[, 2:7], 0.999)

# Configuramos el gráfico para una sola figura
par(mfrow = c(1, 1))

# Graficamos la distancia de Mahalanobis (md) vs. la distancia robusta (rd)
plot(res.mout$md, res.mout$rd, pch = 19,
     xlab = "Distancia de Mahalanobis", ylab = "Distancia robusta",
     main = "Detección de Outliers")

# Dibujamos las líneas de corte (umbral) en ambos ejes
abline(v = res.mout$cutoff, col = "red")
abline(h = res.mout$cutoff, col = "red")

# Identificamos los índices de las observaciones consideradas outliers
ll <- which((res.mout$md > res.mout$cutoff) & (res.mout$rd > res.mout$cutoff))
cat("Número de outliers detectados:", length(ll), "\n")
print(ll)

# Eliminamos los outliers del subconjunto (creamos un nuevo data frame sin ellos)
df_subset_clean <- df_subset[-ll, ]
```


```{r}
m0 <- lm( total_stations_bikes_output ~ population + perc_women + average_age + gross_income + stations_avg_altitude, data = df_subset_clean)
summary(m0)
vif(m0)
#Un vif mayor a 10 indica una colinealidad severa

# 95% IC con el modelo m0r  education = 15 age = 30
#predict( m0 , newdata=data.frame(education = 15, age = 30), interval="prediction")

# Transformaciones para encajar en las hipótesis del modelo lineal
par( mfrow=c(2,2))
plot( m0, id.n=0 )
par( mfrow=c(1,1))
influence.measures(m0)

library(MASS)

boxcox(total_stations_bikes_output ~ population  + average_age + gross_income + stations_avg_altitude, data = df_subset_clean)
#verificacion de valores faltantes:
colSums(is.na(df_subset_clean))
any(df_subset_clean$total_stations_bikes_output <= 0)
summary(df_subset_clean)
boxTidwell(log(total_stations_bikes_output) ~ population + average_age + gross_income + stations_avg_altitude, data = df_subset_clean)
summary(df_subset_clean)
#No permite ejecutar el boxTidwell, probablemente por la elevada dispersion en algunas variables
normalize_minmax <- function(x) {
  scaled_x <- (x - min(x)) / (max(x) - min(x))  # Normalización Min-Max
  return (scaled_x + 1e-6)  # Desplazamos para evitar valores 0
}

df_scaled <- df_subset_clean  # Copia el dataframe original

# Aplicamos normalización Min-Max a todas las variables numéricas excepto total_stations_bikes_output
#df_scaled[, sapply(df_scaled, is.numeric) & names(df_scaled) != "total_stations_bikes_output"] <- 
 # lapply(df_scaled[, sapply(df_scaled, is.numeric) & names(df_scaled) != "total_stations_bikes_output"], normalize_minmax)
m1 <- lm(log(total_stations_bikes_output) ~ population + average_age + gross_income + stations_avg_altitude, data = df_subset_clean)
summary(m1)
vif(m1)

par(mfrow=c(2,2))
plot( m1, id.n=5 )
par(mfrow=c(1,1))

boxTidwell(log(total_stations_bikes_output) ~ population + average_age + gross_income + stations_avg_altitude, data = df_subset_clean)
summary(df_scaled)



#eliminamos la variable perc_women, basados en que no era significativa en el primer modelo
#Aplicamos las transformaciones a income y altitud, sugeridos por boxTidwell
df_subset_clean$gross_income_trans <- df_subset_clean$gross_income^(-0.74953)
df_subset_clean$stations_avg_altitude_trans <- df_subset_clean$stations_avg_altitude^(0.17449)
m1_trans <- lm(log(total_stations_bikes_output) ~ population + average_age +
                 gross_income_trans + stations_avg_altitude_trans,
               data = df_subset_clean)
summary(m1_trans)

par(mfrow=c(2,2))
plot( m1_trans, id.n=5 )
par(mfrow=c(1,1))
#Las observaciones 3,14,27,35,46 pueden ser influyentes
df_subset_clean[c(35, 46, 14, 57), ]
cooksd <- cooks.distance(m1_trans)
plot(cooksd, type="h", main="Distancia de Cook", ylab="Cook’s Distance")
abline(h = 1, col="red", lty=2)

residualPlots( m1_trans )
avPlots(m1, id=list(method=list(cooks.distance(m1),"x"), n = 5))
crPlots(m1)
marginalModelPlots(m1)
marginalModelPlots(m0)
#Lo ideal es que los datos y el modelo coincidan en estas graficas


#Se hará una transformacion polinomial ala variable gross_income, manteniendo la tranformacion de stations_avg_altitude
m2 <- lm(log(total_stations_bikes_output) ~ population + average_age + stations_avg_altitude_trans + poly(gross_income, 2, raw=TRUE), data=df_subset_clean)

summary(m2)
vif(m2)
avPlots(m2, id=list(method=list(cooks.distance(m2),"x"), n = 5))

par(mfrow=c(2,2))
plot( m2, id.n=0 )
par(mfrow=c(1,1))

residualPlots( m2 )
avPlots(m2, id=list(method=list(cooks.distance(m2),"x"), n = 5))
marginalModelPlots(m2)

library(effects)
plot(allEffects(m2, transformation=list(link=log, inverse=exp)))  # Axes name for Y has to be reset to wage

llres<-which(abs(rstudent(m2))>3);llres
llhat<-which(hatvalues(m2)>3*length(coef(m2))/nrow(df_subset_clean));llhat
# Cook's distance es el indicador de influencia
llcoo <- Boxplot(cooks.distance(m2), id=list(n=2, labels=row.names(model.frame(m2))))
influencePlot(m2)


print(llres)
#Aparecen como observaciones influyentes la fila 12 y 14
df_subset_clean[llres,]
ll<-which(row.names(df_subset_clean) %in% c("12", "14"));ll
df_subset_clean[ll,]
df_subset_cleanm3<-df_subset_clean[-ll,]
#En df_subset_cleanm3 se eliminan las observaciones influyentes 12 y 14
m3 <- lm(log(total_stations_bikes_output) ~ population + average_age + stations_avg_altitude_trans + poly(gross_income, 2, raw=TRUE), data=df_subset_cleanm3)
summary(m3)

par(mfrow=c(2,2))
plot( m3, id.n=0 )
par(mfrow=c(1,1))

# El modelo m3 consigue un adjusted R-squared de 0,8373, aunque residuals vs Fitted presenta aun una curvatura
residualPlots( m3 )
avPlots(m3, id=list(method=list(cooks.distance(m3),"x"), n = 7))
marginalModelPlots(m3)
library(effects)
plot(allEffects(m3))

llres<-which(abs(rstudent(m3))>3);llres
llhat<-which(hatvalues(m3)>3*length(coef(m3))/nrow(df));llhat
llcoo<-Boxplot(cooks.distance(m3), id=list(n=2,labels=row.names(df_subset_cleanm3)))
influencePlot(m3, id=list(n=5,method="noteworthy"))

names(df_subset_cleanm3)
